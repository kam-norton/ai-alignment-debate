Multi-agent safety: In robotic systems, the adoption of ML and AI is almost becoming ubiquitous. Needless to say, if these robots and autonomous systems are to cooperate and co-exist with humans and other agents, safety is the most important consideration. 
In autonomous navigation of robots and other systems like self-driving cars, perception of the world is handled through sensors, and these sensors process data fast - often involving ML and AI models to decode the information into useful metrics and a basis on which to produce actions. Safety for this is a huge concern, as these perception systems are at risk of adversarial attacks (people changing stop-signs or data feeds). I'm interested in how we can create a protocol where multiple perception agents can work together (cooperate) to make this perception safer and more robust to adversarial attacks by combining different sensing modes and perception techniques, and having agents collaborate on taking action, to see if it would make their existence safer, or if it would increase concern for events like collusion or deception. 

This leads well into gradual disempowerment: I'd like to understand how the adoption of all of this technology in computer perception and autonomous navigation will impact the engineers of today, specifically ones whose jobs or companies are being enhanced by AI, and what it means for those who need to re-skill themselves to maintain relevance in the field. It would be great if I could come up with a sort of benchmark for measuring the impact of the adoption of AI for computer and autonomous perception in industries. My idea was to see if this benchmark matched or had any correlation with economic trends, and interpret whether this has an impact on the gradual disempowerment of certain groups of people, or figure out who it enables, and who it disempowers, and if this effect is predictable for the future based on past metrics. 